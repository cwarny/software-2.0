{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c29c7c",
   "metadata": {},
   "source": [
    "# Software 2.0\n",
    "\n",
    "In 2017, Andrej Karpathy wrote a seminal [blog post](https://karpathy.medium.com/software-2-0-a64152b37c35) where he introduced the concept of \"software 2.0\". In a nutshell, he argued that machine learning was just another tool in the software engineer's toolbox, and that there shouldn't really be a difference between software engineer and machine learning engineer. The innovation of machine learning is to substitute imperative programming with what I would call \"programming by example\". Since I've tried to make this point to many people over the years, I figured I would try to make it crystal clear with a super simple (and somewhat dumb) example.\n",
    "\n",
    "A friend was recently preparing for a technical interview and he shared with me one of the coding exercises, which was a simple function to convert Roman numerals to integers. For example:\n",
    "- IV -> 4\n",
    "- MCXCIII -> 1193\n",
    "- DXLVIII -> 548\n",
    "- MMMDCXVI -> 3616\n",
    "- XXI -> 21\n",
    "\n",
    "I'll first write code for the \"classic\" way of programming this (\"imperative programming\"). Then I'll contrast it with the \"machine learning way\" or \"programming by example\". Finally, I'll draw some general conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7f270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RomanToInteger:\n",
    "    def __init__(self):\n",
    "        self.mapping = {\n",
    "            'I': 1, \n",
    "            'V': 5, \n",
    "            'X': 10, \n",
    "            'L': 50, \n",
    "            'C': 100, \n",
    "            'D': 500, \n",
    "            'M': 1000\n",
    "        }\n",
    "\n",
    "    def __call__(self, s):\n",
    "        l = len(s)\n",
    "        tot = 0\n",
    "        prev_n = 0\n",
    "        for i in range(l):\n",
    "            current_n = self.mapping[s[i]]\n",
    "            next_n = self.mapping[s[i+1]] if i+1 < l else 0\n",
    "            if current_n >= next_n:\n",
    "                tot += (current_n - prev_n)\n",
    "                prev_n = 0\n",
    "            else:\n",
    "                prev_n = current_n\n",
    "        return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c3572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegerToRoman:\n",
    "    def __init__(self):\n",
    "        self.mapping = {\n",
    "            1: 'I', \n",
    "            5: 'V', \n",
    "            10: 'X', \n",
    "            50: 'L', \n",
    "            100: 'C', \n",
    "            500: 'D', \n",
    "            1000: 'M'\n",
    "        }\n",
    "\n",
    "    def __call__(self, n):\n",
    "        div = 1\n",
    "        while n >= div: div *= 10\n",
    "        div //= 10\n",
    "        out = []\n",
    "        while n:\n",
    "            d = n // div\n",
    "            if d < 4:\n",
    "                o = self.mapping[div]*d\n",
    "            elif d == 4:\n",
    "                o = self.mapping[div] + self.mapping[div*5]\n",
    "            elif d < 9:\n",
    "                o = self.mapping[div*5] + (d-5)*self.mapping[div]\n",
    "            else:\n",
    "                o = self.mapping[div] + self.mapping[div*10]\n",
    "            out.append(o)\n",
    "            n = n % div\n",
    "            div //= 10\n",
    "        return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2d545378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "default_data_path = Path('data')\n",
    "\n",
    "def make_data(splits, path=None, max_num=3999):\n",
    "    if path is None: path = default_data_path\n",
    "    if not path.exists(): path.mkdir()\n",
    "    mapper = IntegerToRoman()\n",
    "    for split, size in splits.items():\n",
    "        with open(path/(split+'.txt'), 'w') as o:\n",
    "            for _ in range(int(size)):\n",
    "                i = random.randint(1, max_num)\n",
    "                r = mapper(i)\n",
    "                o.write(' '.join([str(i), r]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "25f4e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits  = {\n",
    "    'train': 1e6,\n",
    "    'valid': 1e5\n",
    "}\n",
    "\n",
    "make_data(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8fe46ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    list(map(str,range(10)))\n",
    "    + ['I','V','X','L','C','D','M'],\n",
    "    specials=['<bos>', '<pad>', '<eos>']\n",
    ")\n",
    "\n",
    "pad_idx = vocab['<pad>']\n",
    "\n",
    "class NumberDataset(IterableDataset):\n",
    "    valid_targets = ['roman','integer']\n",
    "\n",
    "    def __init__(self, split, target='roman', root=None, extension='txt'):\n",
    "        if root is None: root = default_data_path   \n",
    "        self.url = root/('.'.join([split,extension]))\n",
    "        assert target in self.valid_targets, f'Target needs to be one of {self.valid_targets}'\n",
    "        self.target = target\n",
    "\n",
    "    def __iter__(self):\n",
    "        try:\n",
    "            with open(self.url) as f:\n",
    "                for line in f:\n",
    "                    i, r = line.split()\n",
    "                    yield (i,r) if self.target == 'roman' else (r,i)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "def transform(vocab, x):\n",
    "    seq = ['<bos>'] + list(x) + ['<eos>']\n",
    "    return [vocab[tok] for tok in seq]\n",
    "\n",
    "def untransform(vocab, x):\n",
    "    out = []\n",
    "    for idx in x:\n",
    "        tok = vocab.lookup_token(idx)\n",
    "        if tok == '<bos>': continue\n",
    "        elif tok == '<eos>': return ''.join(out)\n",
    "        else: out.append(tok)\n",
    "    return ''.join(out)\n",
    "\n",
    "def collate(batch):\n",
    "    src_lst, tgt_lst = [], []\n",
    "    for src, tgt in batch:\n",
    "        src, tgt = map(partial(transform, vocab), [src, tgt])\n",
    "        src, tgt = map(torch.tensor, [src, tgt])\n",
    "        src_lst.append(src)\n",
    "        tgt_lst.append(tgt)\n",
    "    return list(map(partial(pad_sequence, padding_value=pad_idx, batch_first=True), [src_lst, tgt_lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "3bf908e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = NumberDataset('train')\n",
    "valid_iter = NumberDataset('valid')\n",
    "train_dl = DataLoader(list(train_iter), batch_size=8, collate_fn=collate)\n",
    "valid_dl = DataLoader(list(valid_iter), batch_size=8, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b6a5169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=100, hidden_size=100, dropout=.2, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.emb(x))\n",
    "        out, (h, c) = self.rnn(x)\n",
    "        return h, c\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=100, hidden_size=100, dropout=.2, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, h, c):\n",
    "        # x is a single token (tensor shape N) \n",
    "        # but we need to feed the RNN a seq (tensor shape Nx1)\n",
    "        # -> need to unsqueeze x on the time dim (dim 1)\n",
    "        x = self.dropout(self.emb(x.unsqueeze(1)))\n",
    "        out, (h, c) = self.rnn(x, (h, c))\n",
    "        out = self.lin(out.squeeze(1)) # we squeeze the time dim back out of existence\n",
    "        return out, h, c\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_proba=.5):\n",
    "        bs = tgt.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.vocab_size\n",
    "        h, c = self.encoder(src)\n",
    "        x = tgt[:,0] # <sos> token\n",
    "        outputs = []\n",
    "        for t in range(1, tgt_len):\n",
    "            out, h, c = self.decoder(x, h, c)\n",
    "            outputs.append(out)\n",
    "            teacher_force = random.random() < teacher_forcing_proba\n",
    "            x = tgt[:,t] if teacher_force else out.argmax(-1)\n",
    "        return torch.stack(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "432b1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "enc = Encoder(vocab_size)\n",
    "dec = Decoder(vocab_size)\n",
    "mdl = Net(enc, dec)\n",
    "opt = optim.Adam(mdl.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "33ae98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mdl, dl, opt, loss_fn):\n",
    "    mdl.train()\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in dl:\n",
    "        opt.zero_grad()\n",
    "        out = mdl(src, tgt)\n",
    "        out_dim = out.size(-1)\n",
    "        assert out.size(1) == tgt.size(1)-1 # we skipped the first element in the output\n",
    "        # collapse seq and batch dims\n",
    "        out = out.view(-1, out_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1) # skip the first element in the ground truth\n",
    "        loss = loss_fn(out, tgt)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "5a23d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(mdl, dl, loss_fn):\n",
    "    mdl.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dl:\n",
    "            out = mdl(src, tgt, 0) # turn off teacher forcing\n",
    "            out_dim = out.size(-1)\n",
    "            out = out.view(-1, out_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "            loss = loss_fn(out, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "1cda703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.008\n",
      "\tValid Loss: 0.000\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.001\n",
      "\tValid Loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(epochs):    \n",
    "    train_loss = train(mdl, train_dl, opt, criterion)\n",
    "    valid_loss = evaluate(mdl, valid_dl, criterion)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(mdl.state_dict(), 'model.pt')\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2f0fb775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 4\n",
      "Predicted: MCL\n",
      "Expected: IV\n",
      "\n",
      "Input: 1193\n",
      "Predicted: MCXCIII\n",
      "Expected: MCXCIII\n",
      "\n",
      "Input: 548\n",
      "Predicted: MDXLVIII\n",
      "Expected: DXLVIII\n",
      "\n",
      "Input: 3616\n",
      "Predicted: MMMDCXVI\n",
      "Expected: MMMDCXVI\n",
      "\n",
      "Input: 21\n",
      "Predicted: MCXX\n",
      "Expected: XXI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tests = [('4','IV'), ('1193', 'MCXCIII'), ('548', 'DXLVIII'), ('3616', 'MMMDCXVI'), ('21', 'XXI')]\n",
    "\n",
    "for src, tgt in tests:\n",
    "    s, t = map(partial(transform, vocab), [src, tgt])\n",
    "    s, t = map(torch.tensor, [s, t])\n",
    "    s, t = map(lambda x: x.unsqueeze(0), [s, t])\n",
    "    logits = mdl(s, t, 0)\n",
    "    pred = logits.squeeze(0).argmax(-1).tolist()\n",
    "    pred = untransform(vocab, pred)\n",
    "    print(f'Input: {src}')\n",
    "    print(f'Predicted: {pred}')\n",
    "    print(f'Expected: {tgt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0cff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
