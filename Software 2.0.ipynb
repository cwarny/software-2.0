{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software 2.0\n",
    "\n",
    "In 2017, Andrej Karpathy wrote a seminal [blog post](https://karpathy.medium.com/software-2-0-a64152b37c35) where he introduced the concept of \"software 2.0\". In a nutshell, he argued that machine learning was just another tool in the software engineer's toolbox, and that there shouldn't really be a difference between software engineer and machine learning engineer. The innovation of machine learning is to substitute imperative programming with what I would call \"programming by example\". Since I've tried to make this point to many people over the years, I figured I would try to make it crystal clear with a super simple (and somewhat dumb) example.\n",
    "\n",
    "A friend was recently preparing for a technical interview and he shared with me one of the coding exercises, which was a simple function to convert Roman numerals to integers. For example:\n",
    "- IV -> 4\n",
    "- MCXCIII -> 1193\n",
    "- DXLVIII -> 548\n",
    "- MMMDCXVI -> 3616\n",
    "- XXI -> 21\n",
    "\n",
    "I'll first write code for the \"classic\" way of programming this (\"imperative programming\"). Then I'll contrast it with the \"machine learning way\" or \"programming by example\". Finally, I'll draw some general conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RomanToInteger:\n",
    "    def __init__(self):\n",
    "        self.mapping = {\n",
    "            'I': 1, \n",
    "            'V': 5, \n",
    "            'X': 10, \n",
    "            'L': 50, \n",
    "            'C': 100, \n",
    "            'D': 500, \n",
    "            'M': 1000\n",
    "        }\n",
    "\n",
    "    def __call__(self, s):\n",
    "        l = len(s)\n",
    "        tot = 0\n",
    "        prev_n = 0\n",
    "        for i in range(l):\n",
    "            current_n = self.mapping[s[i]]\n",
    "            next_n = self.mapping[s[i+1]] if i+1 < l else 0\n",
    "            if current_n >= next_n:\n",
    "                tot += (current_n - prev_n)\n",
    "                prev_n = 0\n",
    "            else:\n",
    "                prev_n = current_n\n",
    "        return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegerToRoman:\n",
    "    def __init__(self):\n",
    "        self.mapping = {\n",
    "            1: 'I', \n",
    "            5: 'V', \n",
    "            10: 'X', \n",
    "            50: 'L', \n",
    "            100: 'C', \n",
    "            500: 'D', \n",
    "            1000: 'M'\n",
    "        }\n",
    "\n",
    "    def __call__(self, n):\n",
    "        div = 1\n",
    "        while n >= div: div *= 10\n",
    "        div //= 10\n",
    "        out = []\n",
    "        while n:\n",
    "            d = n // div\n",
    "            if d < 4:\n",
    "                o = self.mapping[div]*d\n",
    "            elif d == 4:\n",
    "                o = self.mapping[div] + self.mapping[div*5]\n",
    "            elif d < 9:\n",
    "                o = self.mapping[div*5] + (d-5)*self.mapping[div]\n",
    "            else:\n",
    "                o = self.mapping[div] + self.mapping[div*10]\n",
    "            out.append(o)\n",
    "            n = n % div\n",
    "            div //= 10\n",
    "        return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "default_data_path = Path('data')\n",
    "\n",
    "def make_data(splits, path=None, max_num=3999):\n",
    "    if path is None: path = default_data_path\n",
    "    if not path.exists(): path.mkdir()\n",
    "    mapper = IntegerToRoman()\n",
    "    for split, size in splits.items():\n",
    "        with open(path/(split+'.txt'), 'w') as o:\n",
    "            for _ in range(int(size)):\n",
    "                i = random.randint(1, max_num)\n",
    "                r = mapper(i)\n",
    "                o.write(' '.join([str(i), r]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits  = {\n",
    "    'train': 1e6,\n",
    "    'valid': 1e5\n",
    "}\n",
    "\n",
    "make_data(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    list(map(str,range(10)))\n",
    "    + ['I','V','X','L','C','D','M'],\n",
    "    specials=['<bos>', '<pad>', '<eos>']\n",
    ")\n",
    "\n",
    "pad_idx = vocab['<pad>']\n",
    "\n",
    "class NumberDataset(IterableDataset):\n",
    "    valid_targets = ['roman','integer']\n",
    "\n",
    "    def __init__(self, split, target='roman', root=None, extension='txt'):\n",
    "        if root is None: root = default_data_path   \n",
    "        self.url = root/('.'.join([split,extension]))\n",
    "        assert target in self.valid_targets, f'Target needs to be one of {self.valid_targets}'\n",
    "        self.target = target\n",
    "\n",
    "    def __iter__(self):\n",
    "        try:\n",
    "            with open(self.url) as f:\n",
    "                for line in f:\n",
    "                    i, r = line.split()\n",
    "                    yield (i,r) if self.target == 'roman' else (r,i)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "def transform(vocab, x):\n",
    "    seq = ['<bos>'] + list(x) + ['<eos>']\n",
    "    return [vocab[tok] for tok in seq]\n",
    "\n",
    "def untransform(vocab, x):\n",
    "    out = []\n",
    "    for idx in x:\n",
    "        tok = vocab.lookup_token(idx)\n",
    "        if tok == '<bos>': continue\n",
    "        elif tok == '<eos>': return ''.join(out)\n",
    "        else: out.append(tok)\n",
    "    return ''.join(out)\n",
    "\n",
    "def collate(batch):\n",
    "    src_lst, tgt_lst = [], []\n",
    "    for src, tgt in batch:\n",
    "        src, tgt = map(partial(transform, vocab), [src, tgt])\n",
    "        src, tgt = map(torch.tensor, [src, tgt])\n",
    "        src_lst.append(src)\n",
    "        tgt_lst.append(tgt)\n",
    "    return list(map(partial(pad_sequence, padding_value=pad_idx, batch_first=True), [src_lst, tgt_lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = NumberDataset('train')\n",
    "valid_iter = NumberDataset('valid')\n",
    "train_dl = DataLoader(list(train_iter), batch_size=8, collate_fn=collate)\n",
    "valid_dl = DataLoader(list(valid_iter), batch_size=8, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=50, hidden_size=50, dropout=.7, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.emb(x))\n",
    "        out, (h, c) = self.rnn(x)\n",
    "        return h, c\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=50, hidden_size=50, dropout=.7, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, h, c):\n",
    "        # x is a single token (tensor shape N) \n",
    "        # but we need to feed the RNN a seq (tensor shape Nx1)\n",
    "        # -> need to unsqueeze x on the time dim (dim 1)\n",
    "        x = self.dropout(self.emb(x.unsqueeze(1)))\n",
    "        out, (h, c) = self.rnn(x, (h, c))\n",
    "        out = self.lin(out.squeeze(1)) # we squeeze the time dim back out of existence\n",
    "        return out, h, c\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_proba=.5):\n",
    "        bs = tgt.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.vocab_size\n",
    "        h, c = self.encoder(src)\n",
    "        x = tgt[:,0] # <sos> token\n",
    "        outputs = []\n",
    "        for t in range(1, tgt_len):\n",
    "            out, h, c = self.decoder(x, h, c)\n",
    "            outputs.append(out)\n",
    "            teacher_force = random.random() < teacher_forcing_proba\n",
    "            x = tgt[:,t] if teacher_force else out.argmax(-1)\n",
    "        return torch.stack(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwarny/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "enc = Encoder(vocab_size)\n",
    "dec = Decoder(vocab_size)\n",
    "mdl = Net(enc, dec)\n",
    "opt = optim.Adam(mdl.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mdl, dl, opt, loss_fn):\n",
    "    mdl.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, tgt) in enumerate(dl):\n",
    "        opt.zero_grad()\n",
    "        out = mdl(src, tgt)\n",
    "        out_dim = out.size(-1)\n",
    "        assert out.size(1) == tgt.size(1)-1 # we skipped the first element in the output\n",
    "        # collapse seq and batch dims\n",
    "        out = out.view(-1, out_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1) # skip the first element in the ground truth\n",
    "        loss = loss_fn(out, tgt)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if i > 0 and i % 1e4 == 0:\n",
    "            print(f'\\t{i}: {(epoch_loss/(i*src.size(0))):.3f}')\n",
    "    return epoch_loss / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(mdl, dl, loss_fn):\n",
    "    mdl.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dl:\n",
    "            out = mdl(src, tgt, 0) # turn off teacher forcing\n",
    "            out_dim = out.size(-1)\n",
    "            out = out.view(-1, out_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "            loss = loss_fn(out, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\t10000: 0.027\n",
      "\t20000: 0.017\n",
      "\t30000: 0.012\n",
      "\t40000: 0.010\n",
      "\t50000: 0.008\n",
      "\t60000: 0.007\n",
      "\t70000: 0.006\n",
      "\t80000: 0.006\n",
      "\t90000: 0.005\n",
      "\t100000: 0.005\n",
      "\t110000: 0.004\n",
      "\t120000: 0.004\n",
      "\n",
      "\tTrain Loss: 0.033\n",
      "\tValid Loss: 0.000\n",
      "Epoch: 02\n",
      "\t10000: 0.001\n",
      "\t20000: 0.001\n",
      "\t30000: 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-054e82a2e2a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch+1:02}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_valid_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-e84529e2c0e2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(mdl, dl, opt, loss_fn)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# skip the first element in the ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    train_loss = train(mdl, train_dl, opt, criterion)\n",
    "    valid_loss = evaluate(mdl, valid_dl, criterion)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(mdl.state_dict(), 'model.pt')\n",
    "    print(f'\\n\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 4\n",
      "\tPredicted: MCX\n",
      "\tExpected: IV\n",
      "\n",
      "Input: 1193\n",
      "\tPredicted: MCXCIII\n",
      "\tExpected: MCXCIII\n",
      "\n",
      "Input: 548\n",
      "\tPredicted: MDXLVIII\n",
      "\tExpected: DXLVIII\n",
      "\n",
      "Input: 3616\n",
      "\tPredicted: MMMDCXVI\n",
      "\tExpected: MMMDCXVI\n",
      "\n",
      "Input: 21\n",
      "\tPredicted: MMXX\n",
      "\tExpected: XXI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tests = [('4','IV'), ('1193', 'MCXCIII'), ('548', 'DXLVIII'), ('3616', 'MMMDCXVI'), ('21', 'XXI')]\n",
    "\n",
    "for src, tgt in tests:\n",
    "    s, t = map(partial(transform, vocab), [src, tgt])\n",
    "    s, t = map(torch.tensor, [s, t])\n",
    "    s, t = map(lambda x: x.unsqueeze(0), [s, t])\n",
    "    logits = mdl(s, t, 0)\n",
    "    pred = logits.squeeze(0).argmax(-1).tolist()\n",
    "    pred = untransform(vocab, pred)\n",
    "    print(f'Input: {src}')\n",
    "    print(f'\\tPredicted: {pred}')\n",
    "    print(f'\\tExpected: {tgt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.FloatTensor(decoder_dim, encoder_dim).uniform_(-0.1, 0.1))\n",
    "    \n",
    "    def forward(self, q, v):\n",
    "        # queries (q) are the decoder's hidden states (bs, seq_len, decoder_dim)\n",
    "        # values (v) are the encoder's hidden states (bs, seq_len, encoder_dim)\n",
    "        a = (q @ self.w @ v.T)/math.sqrt(q.size(-1))\n",
    "        c = F.softmax(a, 0) @ v # context vector\n",
    "        return c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
