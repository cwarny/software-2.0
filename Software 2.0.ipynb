{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1825b9a",
   "metadata": {},
   "source": [
    "# The economics of software 2.0\n",
    "\n",
    "In 2017, Andrej Karpathy wrote a seminal [blog post](https://karpathy.medium.com/software-2-0-a64152b37c35) where he called neural networks \"software 2.0\". He meant two things: (1) coding neural nets is writing software; (2) neural nets are a new kind of software. The innovation consists of substituting imperative programming with \"programming by example\". Instead of writing in detail the operations a program should go through to process some input and produce some output, you just specify a goal for the program and show it many examples. Watching the examples, the program will then \"write itself\". \n",
    "\n",
    "In his post, Karpathy then goes on to speculate about the consequences of this--an invaluable read. Over the years, I've found myself making Karpathy's point to various people, but I wasn't sure it really landed with them. One problem is that the types of problems we tend to solve with neural nets (computer vision, speech synthesis, natural language understanding, etc.), we never were really close to solving with imperative programming. Many people therefore don't really see that software 1.0 and software 2.0 are substitutes--or fundamentally different ways of doing the same thing: writing software. It feels like they're just different things solving non-overlapping problems. And this is mostly true. But I think the overlaps are going to grow more than we think. \n",
    "\n",
    "Recently, I stumbled upon a simple problem that I thought would illustrate well not only the differences between software 1.0 and software 2.0 but also how they compete with each other in solving problems. I will also use that to illustrate the economics guiding the software 1.0 v software 2.0 choice, and what are the tradeoffs involves in that choice. \n",
    "\n",
    "A friend was preparing for a technical interview on [Leetcode](https://leetcode.com/) and he shared with me one of the problems he had worked on: converting integers into Roman numerals. At present, no one in their right mind would use a neural net to solve this problem (it likely takes longer to code, is slower to run, and consumes more memory and compute). But, in the future, the software 2.0 tooling might be so effective, chips so optimized, and memory and compute so cheap, that it might actually make sense to solve this problem with a neural net. The main reason is that software 2.0 substitutes cheap labor for expensive labor. \n",
    "\n",
    "Assuming neural nets become off-the-shelf commodities, the work involved in programming by example is... collecting examples. As Karpathy puts it, \"most of the active 'software development' takes the form of curating, growing, massaging and cleaning labeled datasets.\" This is arguably more accessible than writing imperative code. Imperative coding essentially requires humans to think like robots. Any sloppiness in reasoning and you get a memory leakage, an infinite loop, or some other unintended consequence. It's merciless, and it takes years to rewire your brain to avoid all the common pitfalls. Curating datasets, in contrast, is much more human-friendly, and therefore the labor pool is larger. \n",
    "\n",
    "The second big advantage of programming by example is that the same neural net architecture can be used to solve different problems. The labor that went into building the neural net can therefore be amortized over several tasks. Each different task still requires one to collect different data (\"software 2.0 development\"), but as we mentioned, this may be much cheaper than writing imperative coding. Software 1.0 is much more \"brittle\" in that sense. The imperative code to convert of integers into Roman numerals is completely different from the imperative code to convert Roman numerals into integers. In contrast, you can repurpose the exact same neural net to solve this new problem. There currently are limits to the versatility of neural nets though. Typically, there are \"problem spaces\" within which a given architecture can be repurposed. For instance, a recurrent neural net can be somewhat seamlessly repurposed for any problem involving converting a sequence of symbols into another sequence of symbols (this is the case I will illustrate in this post). Similarly, without changes to its architecture, the same convolutional neural net can be repurposed for various image classification tasks--whether it is detecting cats in pictures, or evidence of cancer in X-rays. You just have to feed it the right examples (this is not totally true: while the neural net architecture remains the same, you may also need to tweak its hyperparameters). While the versatility of neural nets remains contained within separate problem spaces, the main goal of the machine learning community as a whole is to expand each of those islands and eventually merge them. And in fact, recent developments hint at such a convergence: for instance, the \"transformer\" (a kind of neural net architecture) recently crossed over from the natural language processing space into the image processing space--a unification of sorts.\n",
    "\n",
    "There are downsides to programming by example though. Despite future optimizations, it might remain on average less efficient than imperative coding. More importantly, a piece of software that wrote itself via neural nets is fundamentally probabilistic: you cannot 100% guarantee that it will always produce the correct output. In theory, you can offer that guarantee with software 1.0, although people rarely go through the trouble of proving correctness of an imperative codebase. That said, those two downsides pale in comparison to the above advantages (cheap development and versatility).\n",
    "\n",
    "Before we dive into our little example to illustrate all of the above, let me quickly reiterate the point I'm trying to make here: imperative programming and programming by example, while they currently may seem to address non-overlapping problems, might be more directly competing over problems in the future. Using neural nets to convert integers into Roman numerals (or vice versa) might seem silly, but I think it makes the point crystal clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f75944",
   "metadata": {},
   "source": [
    "## Converting integers into Roman numerals\n",
    "\n",
    "The description of the problem is fairly straigtforward. I'll just show a few example conversions:\n",
    "\n",
    "- 4 -> IV\n",
    "- 1193 -> MCXCIII\n",
    "- 548 -> DXLVIII\n",
    "- 3616 -> MMMDCXVI\n",
    "- 21 -> XXI\n",
    "\n",
    "Most people are familiar with Roman numerals. Unlike Arabic numerals, the value of a Roman numerical symbol does not depend on its position within a number. The symbol \"3\" in \"300\" has a different \"value\" than in \"30\". In the Roman system, \"V\" means 5--regardless of its position in the number. Roman symbols may also be locally subtracted as you parse them in order to figure out the final number. For instance, \"XIV\" = \"X + (V - I)\" = 14. Arabic symbols are just added: 14 = 10 + 4. \n",
    "\n",
    "Generally speaking, Arabic numerals are more compact, easier to compare, and way more convenient for doing arithmetic. Finally, it scales way better to represent large numbers. The Roman system would have to just keep adding symbols, making the whole thing more and more unwieldy and unparseable (thankfully, Romans didn't seem to need to handle numbers larger than 3999).\n",
    "\n",
    "Let's write the imperative code to convert integers into Roman numerals. Later, we'll contrast this with the \"machine learning way\" of doing this. For both approaches, I'll use the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23330af2",
   "metadata": {},
   "source": [
    "### The imperative way\n",
    "\n",
    "Let's first define a mapping between Roman numerals and their corresponding integers, and then reverse the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0498dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2i = {\n",
    "    'I': 1, \n",
    "    'V': 5, \n",
    "    'X': 10, \n",
    "    'L': 50, \n",
    "    'C': 100, \n",
    "    'D': 500, \n",
    "    'M': 1000\n",
    "}\n",
    "\n",
    "i2r = {v:k for k,v in r2i.items()} # reverse the mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04343d7",
   "metadata": {},
   "source": [
    "Now onto the function itself. The main trick is to know how to repeatedly extract the most significant digit of the integer. To do that, you need to divide it by the nearest power of 10 that is still lower than that integer. The quotient of that division will be the most significant digit. Then you map it to its Roman equivalent with some simple rules. You then move on to the next most significant digit. In practice, you replace the original integer with the remainder of the above divison and compute the most significant digit of that new integer (don't forget to update the divisor to be an order of magnitude lower). Honestly, I don't think this problem is straightforward for most people, which is probably why Leetcode considers it medium in difficulty (when I checked, only 60% of the submissions on the website were correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "037d4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_to_roman(n):\n",
    "    div = 1\n",
    "    while n >= div: div *= 10\n",
    "    div //= 10\n",
    "    out = []\n",
    "    while n:\n",
    "        d = n // div # get most significant digit via floor division by a power of 10\n",
    "        if d < 4:\n",
    "            o = i2r[div]*d\n",
    "        elif d == 4:\n",
    "            o = i2r[div] + i2r[div*5]\n",
    "        elif d < 9:\n",
    "            o = i2r[div*5] + (d-5)*i2r[div]\n",
    "        else:\n",
    "            o = i2r[div] + i2r[div*10]\n",
    "        out.append(o)\n",
    "        n = n % div # the new integer is the remainder\n",
    "        div //= 10\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353523ed",
   "metadata": {},
   "source": [
    "Let's make a quick test set for sanity check purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8ab420",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    ('4', 'IV'),\n",
    "    ('1193', 'MCXCIII'),\n",
    "    ('548', 'DXLVIII'),\n",
    "    ('3616', 'MMMDCXVI'),\n",
    "    ('21', 'XXI')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b38afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "for src,tgt in test_set:\n",
    "    assert integer_to_roman(int(src)) == tgt\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b11e3",
   "metadata": {},
   "source": [
    "### Programming by example\n",
    "\n",
    "As discussed, programming by example requires collecting examples. So let's assume we paid someone to do just that. For instance, that person asked 100 different people to come up with 20 examples. Of those 2000 examples, half will be used for the neural net to \"write itself\". The other half will be used to check how well it's doing. If there is room for improvement, typically it's because showing the various examples once is not enough. Sometimes you have to go through the same examples a few times for the neural net to learn (we call these repetitions \"epochs\"). Let's further keep 100 examples completely aside. At the end, when we're confident the neural net has learned to solve the problem, we use the 100 examples held out to get an estimate of its accuracy. Remember: we mentioned that a downside of software 2.0 is that it's probabilistic. The test set allows us to estimate how reliable the software is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906dccb2",
   "metadata": {},
   "source": [
    "Let's synthetically generate some examples to simulate the data collection effort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38765561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "default_data_path = Path('data')\n",
    "\n",
    "def make_data(splits, path=None, max_num=3999):\n",
    "    if path is None: path = default_data_path\n",
    "    if not path.exists(): path.mkdir()\n",
    "    for split, size in splits.items():\n",
    "        with open(path/(split+'.txt'), 'w') as o:\n",
    "            for _ in range(int(size)):\n",
    "                i = random.randint(1, max_num)\n",
    "                r = integer_to_roman(i)\n",
    "                o.write(' '.join([str(i), r]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53e67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits  = {\n",
    "    'train': 1e3,\n",
    "    'valid': 1e3,\n",
    "    'test': 1e2\n",
    "}\n",
    "\n",
    "make_data(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca8bc6",
   "metadata": {},
   "source": [
    "To code up the neural net, I'll use PyTorch--a popular Python framework for deep learning. The first thing to do is to write classes to process the data and serve it to the neural net in the appropriate format. The `NumberDataset` class reads the examples from a file. Each line in the file consists in example pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61dfdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3250 MMMCCL\r\n",
      "1298 MCCXCVIII\r\n",
      "758 DCCLVIII\r\n",
      "784 DCCLXXXIV\r\n",
      "2738 MMDCCXXXVIII\r\n",
      "765 DCCLXV\r\n",
      "1845 MDCCCXLV\r\n",
      "3723 MMMDCCXXIII\r\n",
      "1104 MCIV\r\n",
      "2470 MMCDLXX\r\n"
     ]
    }
   ],
   "source": [
    "!head data/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c7c16",
   "metadata": {},
   "source": [
    "We are going to consider both the Arabic and Roman numerals as arbitrary symbols, which together form a common vocabulary. Those symbols will therefore be treated as characters. For instance, \"794\" is a string of the following symbols: \"7\", \"9\", \"4\". Similarly, \"VIII\" is a string of the following symbols: \"V\", \"I\", \"I\", and \"I\". All these symbols (\"4\", \"7\", \"9\", \"I\", \"V\") are part of our vocabulary. However, neural nets work with numbers, not strings. So you can't just serve the net with strings of characters. This is where the `Processor` class comes in: it maps the symbols in our vocabulary to unique integers. And *that*'s what we feed the neural net. One last thing: neural nets also typically expect their inputs to come in *batches*. Since, at the end of the day, neural nets are just a series of matrix multiplications with some nonlinearities peppered here and there, processing is made more efficient by using batches of numbers. That's the role of the `collate` function, which forms nice \"boxes\" of integers to feed the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2cd332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwarny/opt/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "\n",
    "class NumberDataset(Dataset):\n",
    "    valid_targets = ['roman','integer']\n",
    "\n",
    "    def __init__(self, lst, processor=None, target='roman'):\n",
    "        assert target in self.valid_targets, f'Target needs to be one of {self.valid_targets}'\n",
    "        self.target = target\n",
    "        self.processor = processor\n",
    "        self.lst = lst\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        i,r = self.lst[i]\n",
    "        t = (i,r) if self.target == 'roman' else (r,i)\n",
    "        return list(map(self.processor.process, t))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lst)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, fn, root=None, extension='txt', **kwargs):\n",
    "        if root is None: root = default_data_path   \n",
    "        url = root/('.'.join([fn,extension]))\n",
    "        with open(url) as f: lines = [line.split() for line in f]\n",
    "        return cls(lines, **kwargs)\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def process(self, x):\n",
    "        seq = ['<bos>'] + list(x) + ['<eos>']\n",
    "        return [self.vocab[tok] for tok in seq]\n",
    "\n",
    "    def deprocess(self, x):\n",
    "        out = []\n",
    "        for idx in x:\n",
    "            tok = self.vocab.lookup_token(idx)\n",
    "            if tok == '<bos>': continue\n",
    "            elif tok == '<eos>': return ''.join(out)\n",
    "            else: out.append(tok)\n",
    "        return ''.join(out)\n",
    "\n",
    "def collate(batch, max_len=20, pad_idx=1):\n",
    "    src_lst, tgt_lst = [], []\n",
    "    for src, tgt in batch:\n",
    "        src, tgt = map(torch.tensor, [src, tgt])\n",
    "        src_lst.append(src)\n",
    "        tgt_lst.append(tgt)\n",
    "    src_lst[0] = nn.ConstantPad1d((0, max_len-src_lst[0].size(0)), pad_idx)(src_lst[0])\n",
    "    tgt_lst[0] = nn.ConstantPad1d((0, max_len-tgt_lst[0].size(0)), pad_idx)(tgt_lst[0])\n",
    "    return list(map(partial(pad_sequence, padding_value=pad_idx, batch_first=True), [src_lst, tgt_lst]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f8652",
   "metadata": {},
   "source": [
    "Let's now build our vocabulary, and then our datasets. PyTorch's `DataLoader` class just takes a dataset and serves batches from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e3f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "vocab = build_vocab_from_iterator(\n",
    "    list(map(str,range(10))) # Arabic symbols\n",
    "    + ['I','V','X','L','C','D','M'], # Roman symbols\n",
    "    specials=['<bos>', '<pad>', '<eos>'] # Special symbols\n",
    ")\n",
    "vocab_size = len(vocab)\n",
    "pad_idx = vocab['<pad>']\n",
    "collate_fn = partial(collate, pad_idx=pad_idx, max_len=max_len)\n",
    "proc = Processor(vocab)\n",
    "train_ds = NumberDataset.from_file('train', processor=proc)\n",
    "valid_ds = NumberDataset.from_file('valid', processor=proc)\n",
    "train_dl = DataLoader(train_ds, batch_size=10, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=10, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1bd9a3",
   "metadata": {},
   "source": [
    "Now on to the meaty stuff. We'll use a sequence-to-sequence network directly inspired by the classic [2015 Bahdanau et al. paper](https://arxiv.org/pdf/1409.0473.pdf). The architecture has three components: (1) an encoder, (2) a decoder, and (3) an attention mechanism in between. I won't go into the details of these models, as the Internet is already replete with detailed explanations of those models. Essentially, an encoder converts each symbol in a sequence into a multidimensional vector. A vector is simply a way to describe some entity. The more dimensions this vector has, the more granular the description can be. For instance the vector [\"6'1\", \"brown\", \"75kg\"] is a very rough description of myself that only encompasses the three dimensions of height, hair color, and weight. In this case, the \"entity\" is a person. In the case of our little problem, the \"entity\" is a numerical symbol. In both cases, the entity can be representated by a vector of numbers. Vectors are modular: if one of the descriptions in your vector is a non-number entity, it can itself be represented as a vector, and you just expand the original vector. For instance, \"brown\" is not a number, but \"brown\" can be represented as a vector of three numbers (intensities of red, green and blue), so that the final vector representing me is [\"6'1\", \"red intensity of brown\", \"green intensity of brown\", \"blue intensity of brown\", \"75kg\"]. Any non-number entity can eventually be represented by a vector of numbers. Were you to expand the vector, you could cram more information into it.\n",
    "\n",
    "Anyways, the encoder simply converts a string of symbols into an array of vectors that capture information about those symbols, just like [\"6'1\", \"brown\", \"75kg\"] captures information about myself. The attention module then \"summarizes\" the information across those input symbols into a single vector to feed into the decoder. As the name implies, the decoder does the reverse process: it takes a vector as input and produces a symbol. In our case, it will produce a sequence of Roman numerical symbols. To get the decoder started, we need to feed it a special vector whose meaning is \"start of the sequence\". If you look back at the code we use to build our vocabulary, that's the symbol \"<bos>\". But on top of that seed vector, we need to influence the decoder with *some* information from the encoded input. The decoder needs to \"look\" at the encoded input to guide its production of symbols. Every time it spits out a new symbol, the decoder needs to pay attention to the encoded input. Say the input is \"XIV\" and the decoder has already produced the symbol \"1\". Upon producing the next symbol, it probably needs to pay most attention to the second and third vectors in the encoded input (vectors representing \"I\" and \"V\"). That's because the decoder needs to subtract one from the other. This process of \"paying attention to different parts of the input\" is what's known as... an attention mechanism. That's the last component of our architecture. In practice, the decoder needs to \"summarize\" the information it pays attention to into a single vector. The way it does that is by doing a weighted sum of the encoded input vectors. In the case where the decoder is trying to produce \"4\" in the sequence \"14\", it will likely learn to give a low weight to \"X\" in the input and a high weight to \"I\" and \"V\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e67ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.project = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        x = self.dropout(self.emb(x))\n",
    "        h, h_last = self.rnn(x)\n",
    "        h_last = h_last.permute(1,0,2).contiguous().view(bs, -1) # (bs,hidden_dim*2)\n",
    "        h, h_last = map(self.project, [h, h_last])\n",
    "        h_last = h_last.unsqueeze(0)\n",
    "        return h, h_last # (bs,seq_len,hidden_dim), (1,bs,hidden_dim)\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.decoder_hidden_dim = torch.tensor(decoder_hidden_dim)\n",
    "        self.w = nn.Parameter(torch.FloatTensor(decoder_hidden_dim, encoder_hidden_dim).uniform_(-0.1, 0.1))\n",
    "    \n",
    "    def forward(self, query, values):\n",
    "        score = (query.unsqueeze(1) @ self.w @ values.permute(0,2,1))/torch.sqrt(self.decoder_hidden_dim)\n",
    "        attention_weights = F.softmax(score, 1)\n",
    "        context = attention_weights @ values\n",
    "        return context\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim*2, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, y, h_in, context):\n",
    "        y = self.dropout(self.emb(y.unsqueeze(1)))\n",
    "        y = torch.cat([context, y], -1) # (bs,1,hidden_dim*2)\n",
    "        h, h_last = self.rnn(y, h_in) # (bs,1,hidden_dim), (1,bs,hidden_dim)\n",
    "        return h.squeeze(1), h_last\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=20, dropout=.5):\n",
    "        super().__init__()\n",
    "        self.encode = Encoder(vocab_size, hidden_dim, dropout)\n",
    "        self.attend = Attention(hidden_dim, hidden_dim)\n",
    "        self.decode = Decoder(vocab_size, hidden_dim, dropout)\n",
    "        self.project = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_proba=.5):\n",
    "        bs, tgt_len = tgt.shape\n",
    "        h, h_last = self.encode(src)\n",
    "        s = h_last.squeeze(0) # (bs,hidden_dim)\n",
    "        y = tgt[:,0]\n",
    "        logits = []\n",
    "        for t in range(1, tgt_len):\n",
    "            context = self.attend(s, h) # context: (bs,1,hidden_dim)\n",
    "            s, h_last = self.decode(y, h_last, context)\n",
    "            logit = self.project(s)\n",
    "            logits.append(logit)\n",
    "            teacher_force = random.random() < teacher_forcing_proba\n",
    "            y = tgt[:,t] if teacher_force else logit.argmax(-1)\n",
    "        return torch.stack(logits, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e6c34",
   "metadata": {},
   "source": [
    "Let's now write some functions to train and evaluate our model. It's useful to distinguish here between the loss function (also known as \"criterion\") and the metric function. While they both give us a sense of performance, the former is used by the neural net to guide its \"self-writing\" process (technically known as \"gradient descent\") and the latter is used by the human supervisor to evaluate how well the self-writing software is doing, like a foreman overseeing a worker in the factories of old. The criterion is typically more amenable to the gradient descent process but not easily interpretable by the foreman. The metric function, in turn, is more intuitive. In our case, the criterion is cross-entropy and the metric is accuracy (how many sequences does the software get right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f3c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Acc:\n",
    "    def __init__(self, ignore_index=1):\n",
    "        self.ignore_index = ignore_index\n",
    "    \n",
    "    def __call__(self, pred, tgt):\n",
    "        # both pred and tgt have shape (bs,seq_len)\n",
    "        mask = tgt != self.ignore_index\n",
    "        pred *= mask\n",
    "        tgt *= mask\n",
    "        correct = torch.eq(pred, tgt).all(1).sum()\n",
    "        return correct.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fdafc8",
   "metadata": {},
   "source": [
    "The `fit` function encapsulates both the training process and the evaluation process. It repeats both processes until the neural net seems to have converged to a decent level of performance, which the foreman (us) can observe via the metric. The number of epochs just captures how many times the software goes through the same examples until it gets them right. The `patience` parameter is used to decide when to stop training--essentially it causes the software to stop training when its stops making any progress for a few epochs. When the software is done writing itself, the `fit` function spits out the final software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2fdfb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mdl, dl, opt, loss_fn, metric):\n",
    "    mdl.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for i, (src, tgt) in enumerate(dl):\n",
    "        opt.zero_grad()\n",
    "        out = mdl(src, tgt)\n",
    "        bs, seq_len, out_dim = out.shape\n",
    "        assert out.size(1) == tgt.size(1)-1 # we skipped the first element in the output\n",
    "        # collapse seq and batch dims\n",
    "        out = out.view(-1, out_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1) # skip the first element in the ground truth\n",
    "        loss = loss_fn(out, tgt)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "        pred = out.argmax(-1)\n",
    "        m = metric(pred.view(bs, -1), tgt.view(bs, -1))\n",
    "        correct += m\n",
    "        if i > 0 and i % 1e4 == 0:\n",
    "            print(f'\\t{i}: {epoch_loss/i:.3f}')\n",
    "    n = (i+1)*bs\n",
    "    return epoch_loss/n, correct/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e3e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(mdl, dl, loss_fn, metric):\n",
    "    mdl.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(dl):\n",
    "            out = mdl(src, tgt, teacher_forcing_proba=0) # turn off teacher forcing\n",
    "            bs, seq_len, out_dim = out.shape\n",
    "            out = out.view(-1, out_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "            loss = loss_fn(out, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "            pred = out.argmax(-1)\n",
    "            m = metric(pred.view(bs, -1), tgt.view(bs, -1))\n",
    "            correct += m\n",
    "    n = (i+1)*bs\n",
    "    return epoch_loss/n, correct/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d14b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def fit(epochs, mdl, train_dl, valid_dl, opt, criterion, metric, patience=2):\n",
    "    fmt = lambda x: f'{x:.3f}'\n",
    "    best_valid_loss = float('inf')\n",
    "    best_mdl = None\n",
    "    irritation = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch+1:02}')\n",
    "        train_loss, train_metric = train(mdl, train_dl, opt, criterion, metric)\n",
    "        valid_loss, valid_metric = evaluate(mdl, valid_dl, criterion, metric)\n",
    "        print('\\t' + json.dumps({\n",
    "            'train': {\n",
    "                'loss': fmt(train_loss),\n",
    "                'metric': fmt(train_metric)\n",
    "            },\n",
    "            'valid': {\n",
    "                'loss': fmt(valid_loss),\n",
    "                'metric': fmt(valid_metric)\n",
    "            }\n",
    "        }, indent=4))\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_mdl = mdl\n",
    "            torch.save(mdl.state_dict(), 'model.pt')\n",
    "            irritation = 0\n",
    "        else:\n",
    "            irritation += 1\n",
    "            if irritation == patience: break\n",
    "    return best_mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53970d",
   "metadata": {},
   "source": [
    "Let's instantiate our model. The `opt` object is the thing that will run the gradient descent process for us. In other words, it is what manages the neural net's \"self-writing\" process. This is also where we pick several so-called \"hyperparameters\" (`hidden_dim`, `dropout`, `lr`). Without going into the details, they determine either some structural elements of the neural net's architecture (like the size of the vectors representing the symbols) or elements of the training process (like the rate at which the software updates itself based on additional examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c75fc846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwarny/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "i2r_mdl = Seq2SeqWithAttention(vocab_size, hidden_dim=30, dropout=.3)\n",
    "opt = optim.Adam(i2r_mdl.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "metric = Acc(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e703e5",
   "metadata": {},
   "source": [
    "Time to fit! (Only displaying the criterion and metric values at the last epoch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cc3ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.207\",\n",
      "        \"metric\": \"0.000\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.162\",\n",
      "        \"metric\": \"0.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 02\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.151\",\n",
      "        \"metric\": \"0.000\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.143\",\n",
      "        \"metric\": \"0.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 03\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.133\",\n",
      "        \"metric\": \"0.001\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.126\",\n",
      "        \"metric\": \"0.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 04\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.122\",\n",
      "        \"metric\": \"0.001\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.117\",\n",
      "        \"metric\": \"0.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 05\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.116\",\n",
      "        \"metric\": \"0.003\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.127\",\n",
      "        \"metric\": \"0.003\"\n",
      "    }\n",
      "}\n",
      "Epoch: 06\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.111\",\n",
      "        \"metric\": \"0.005\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.101\",\n",
      "        \"metric\": \"0.007\"\n",
      "    }\n",
      "}\n",
      "Epoch: 07\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.106\",\n",
      "        \"metric\": \"0.001\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.124\",\n",
      "        \"metric\": \"0.008\"\n",
      "    }\n",
      "}\n",
      "Epoch: 08\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.102\",\n",
      "        \"metric\": \"0.004\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.099\",\n",
      "        \"metric\": \"0.010\"\n",
      "    }\n",
      "}\n",
      "Epoch: 09\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.099\",\n",
      "        \"metric\": \"0.008\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.094\",\n",
      "        \"metric\": \"0.011\"\n",
      "    }\n",
      "}\n",
      "Epoch: 10\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.093\",\n",
      "        \"metric\": \"0.013\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.091\",\n",
      "        \"metric\": \"0.015\"\n",
      "    }\n",
      "}\n",
      "Epoch: 11\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.091\",\n",
      "        \"metric\": \"0.011\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.085\",\n",
      "        \"metric\": \"0.025\"\n",
      "    }\n",
      "}\n",
      "Epoch: 12\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.086\",\n",
      "        \"metric\": \"0.014\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.086\",\n",
      "        \"metric\": \"0.027\"\n",
      "    }\n",
      "}\n",
      "Epoch: 13\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.084\",\n",
      "        \"metric\": \"0.027\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.074\",\n",
      "        \"metric\": \"0.047\"\n",
      "    }\n",
      "}\n",
      "Epoch: 14\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.080\",\n",
      "        \"metric\": \"0.021\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.072\",\n",
      "        \"metric\": \"0.060\"\n",
      "    }\n",
      "}\n",
      "Epoch: 15\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.077\",\n",
      "        \"metric\": \"0.037\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.067\",\n",
      "        \"metric\": \"0.101\"\n",
      "    }\n",
      "}\n",
      "Epoch: 16\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.074\",\n",
      "        \"metric\": \"0.041\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.074\",\n",
      "        \"metric\": \"0.076\"\n",
      "    }\n",
      "}\n",
      "Epoch: 17\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.070\",\n",
      "        \"metric\": \"0.057\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.062\",\n",
      "        \"metric\": \"0.134\"\n",
      "    }\n",
      "}\n",
      "Epoch: 18\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.067\",\n",
      "        \"metric\": \"0.073\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.056\",\n",
      "        \"metric\": \"0.141\"\n",
      "    }\n",
      "}\n",
      "Epoch: 19\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.064\",\n",
      "        \"metric\": \"0.081\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.062\",\n",
      "        \"metric\": \"0.111\"\n",
      "    }\n",
      "}\n",
      "Epoch: 20\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.061\",\n",
      "        \"metric\": \"0.092\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.053\",\n",
      "        \"metric\": \"0.152\"\n",
      "    }\n",
      "}\n",
      "Epoch: 21\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.060\",\n",
      "        \"metric\": \"0.101\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.055\",\n",
      "        \"metric\": \"0.151\"\n",
      "    }\n",
      "}\n",
      "Epoch: 22\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.056\",\n",
      "        \"metric\": \"0.116\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.049\",\n",
      "        \"metric\": \"0.207\"\n",
      "    }\n",
      "}\n",
      "Epoch: 23\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.055\",\n",
      "        \"metric\": \"0.131\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.050\",\n",
      "        \"metric\": \"0.180\"\n",
      "    }\n",
      "}\n",
      "Epoch: 24\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.056\",\n",
      "        \"metric\": \"0.131\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.045\",\n",
      "        \"metric\": \"0.230\"\n",
      "    }\n",
      "}\n",
      "Epoch: 25\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.049\",\n",
      "        \"metric\": \"0.171\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.040\",\n",
      "        \"metric\": \"0.322\"\n",
      "    }\n",
      "}\n",
      "Epoch: 26\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.049\",\n",
      "        \"metric\": \"0.191\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.037\",\n",
      "        \"metric\": \"0.334\"\n",
      "    }\n",
      "}\n",
      "Epoch: 27\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.046\",\n",
      "        \"metric\": \"0.197\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.036\",\n",
      "        \"metric\": \"0.329\"\n",
      "    }\n",
      "}\n",
      "Epoch: 28\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.043\",\n",
      "        \"metric\": \"0.234\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.032\",\n",
      "        \"metric\": \"0.435\"\n",
      "    }\n",
      "}\n",
      "Epoch: 29\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.039\",\n",
      "        \"metric\": \"0.291\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.029\",\n",
      "        \"metric\": \"0.512\"\n",
      "    }\n",
      "}\n",
      "Epoch: 30\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.037\",\n",
      "        \"metric\": \"0.333\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.033\",\n",
      "        \"metric\": \"0.495\"\n",
      "    }\n",
      "}\n",
      "Epoch: 31\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.034\",\n",
      "        \"metric\": \"0.369\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.024\",\n",
      "        \"metric\": \"0.586\"\n",
      "    }\n",
      "}\n",
      "Epoch: 32\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.033\",\n",
      "        \"metric\": \"0.395\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.021\",\n",
      "        \"metric\": \"0.671\"\n",
      "    }\n",
      "}\n",
      "Epoch: 33\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.032\",\n",
      "        \"metric\": \"0.415\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.027\",\n",
      "        \"metric\": \"0.567\"\n",
      "    }\n",
      "}\n",
      "Epoch: 34\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.029\",\n",
      "        \"metric\": \"0.460\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.023\",\n",
      "        \"metric\": \"0.638\"\n",
      "    }\n",
      "}\n",
      "Epoch: 35\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.027\",\n",
      "        \"metric\": \"0.508\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.020\",\n",
      "        \"metric\": \"0.738\"\n",
      "    }\n",
      "}\n",
      "Epoch: 36\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.028\",\n",
      "        \"metric\": \"0.472\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.030\",\n",
      "        \"metric\": \"0.604\"\n",
      "    }\n",
      "}\n",
      "Epoch: 37\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.027\",\n",
      "        \"metric\": \"0.520\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.020\",\n",
      "        \"metric\": \"0.701\"\n",
      "    }\n",
      "}\n",
      "Epoch: 38\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.024\",\n",
      "        \"metric\": \"0.559\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.015\",\n",
      "        \"metric\": \"0.782\"\n",
      "    }\n",
      "}\n",
      "Epoch: 39\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.024\",\n",
      "        \"metric\": \"0.573\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.014\",\n",
      "        \"metric\": \"0.807\"\n",
      "    }\n",
      "}\n",
      "Epoch: 40\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.020\",\n",
      "        \"metric\": \"0.626\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.014\",\n",
      "        \"metric\": \"0.785\"\n",
      "    }\n",
      "}\n",
      "Epoch: 41\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.021\",\n",
      "        \"metric\": \"0.618\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.021\",\n",
      "        \"metric\": \"0.766\"\n",
      "    }\n",
      "}\n",
      "Epoch: 42\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.019\",\n",
      "        \"metric\": \"0.668\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.010\",\n",
      "        \"metric\": \"0.848\"\n",
      "    }\n",
      "}\n",
      "Epoch: 43\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.019\",\n",
      "        \"metric\": \"0.688\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.012\",\n",
      "        \"metric\": \"0.835\"\n",
      "    }\n",
      "}\n",
      "Epoch: 44\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.018\",\n",
      "        \"metric\": \"0.669\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.009\",\n",
      "        \"metric\": \"0.893\"\n",
      "    }\n",
      "}\n",
      "Epoch: 45\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.018\",\n",
      "        \"metric\": \"0.697\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.014\",\n",
      "        \"metric\": \"0.821\"\n",
      "    }\n",
      "}\n",
      "Epoch: 46\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.015\",\n",
      "        \"metric\": \"0.746\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.007\",\n",
      "        \"metric\": \"0.942\"\n",
      "    }\n",
      "}\n",
      "Epoch: 47\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.017\",\n",
      "        \"metric\": \"0.711\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.013\",\n",
      "        \"metric\": \"0.883\"\n",
      "    }\n",
      "}\n",
      "Epoch: 48\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.018\",\n",
      "        \"metric\": \"0.686\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.012\",\n",
      "        \"metric\": \"0.847\"\n",
      "    }\n",
      "}\n",
      "Epoch: 49\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.013\",\n",
      "        \"metric\": \"0.782\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.020\",\n",
      "        \"metric\": \"0.822\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "best_i2r_mdl = fit(100, i2r_mdl, train_dl, valid_dl, opt, criterion, metric, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba0b741",
   "metadata": {},
   "source": [
    "The validation metric is good (>90%) but not great. For a better sense of the confidence we should put in this piece of probabilistic software, let's run it on our held-out test set of 100 examples unseen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0b2c07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n"
     ]
    }
   ],
   "source": [
    "test_ds = NumberDataset.from_file('test', processor=proc)\n",
    "test_dl = DataLoader(test_ds, batch_size=5, collate_fn=collate_fn)\n",
    "_, test_metric = evaluate(best_i2r_mdl, test_dl, criterion, metric)\n",
    "print(test_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620b4a9",
   "metadata": {},
   "source": [
    "About 90%. Not the greatest. There are various reasons for that. First of all, we only used 1000 examples, which in the realm of deep learning is extremely small. If we collect additional examples, we will reach close to 100% pretty quickly. The right level of performance depends on the use case. The various cost savings of software 2.0 might be worth the accuracy hit. Of course, with this silly use case, a 10% accuracy hit does not make any sense. (And again, the whole idea of using neural nets for this use case does not make much sense, this is just for illustration purposes.)\n",
    "\n",
    "Let's try a few examples to see the software in action. For that we build a `predict` function that wraps the software with some light code needed to serve the input and to format the output. We show the results with our little test set. In parenthesis is the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b6b4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mdl, tests, proc, collate):\n",
    "    mdl.eval()\n",
    "    tests = list(zip(tests,tests))\n",
    "    test_ds = NumberDataset(tests, processor=proc)\n",
    "    test_dl = DataLoader(test_ds, batch_size=len(tests), collate_fn=collate)\n",
    "    with torch.no_grad():\n",
    "        for src,tgt in test_dl:\n",
    "            out = mdl(src, tgt, teacher_forcing_proba=0)\n",
    "            bs, seq_len, out_dim = out.shape\n",
    "            out = out.view(-1, out_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "            pred = out.argmax(-1)\n",
    "            return [proc.deprocess(seq) for seq in pred.view(bs, -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c2e4cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 -> I (IV)\n",
      "1193 -> MCXCIII (MCXCIII)\n",
      "548 -> DXLVIII (DXLVIII)\n",
      "3616 -> MMMDCXVI (MMMDCXVI)\n",
      "21 -> CXX (XXI)\n"
     ]
    }
   ],
   "source": [
    "to_predict = [i for i,r in test_set]\n",
    "preds = predict(best_i2r_mdl, to_predict, proc, collate_fn)\n",
    "for (i,r),pred in zip(test_set,preds):\n",
    "    print(f'{i} -> {pred} ({r})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af2ffa",
   "metadata": {},
   "source": [
    "## Converting Roman numerals into integers\n",
    "\n",
    "I mentioned that versatility is another major benefit of software 2.0. Let's illustrate that by using the exact same neural net architecture to do a different but related task: converting Roman numerals into integers (instead of the other way around). To drive the point home, I first write the equivalent imperative program. Notice how completely different it is from the imperative program that converts integers into Roman numerals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b99ad228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roman_to_integer(s):\n",
    "    l = len(s)\n",
    "    tot = 0\n",
    "    prev_n = 0\n",
    "    for i in range(l):\n",
    "        current_n = r2i[s[i]]\n",
    "        next_n = r2i[s[i+1]] if i+1 < l else 0\n",
    "        if current_n >= next_n:\n",
    "            tot += (current_n - prev_n)\n",
    "            prev_n = 0\n",
    "        else:\n",
    "            prev_n = current_n\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f234465",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert roman_to_integer('MCXCIII') == 1193"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6812b",
   "metadata": {},
   "source": [
    "This was easier for me to code, but still took me some time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818568bf",
   "metadata": {},
   "source": [
    "Now let's turn back to our neural net. This is where software 2.0 really shines. All I have to do to repurpose my neural net is to feed it different data. I don't need to code anything. The only change I make is to add `target='integer'` when reading the data. This ensures that the examples are served in such a way that the neural net learns to produce integers from Roman numerals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7611b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.234\",\n",
      "        \"metric\": \"0.000\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.200\",\n",
      "        \"metric\": \"0.001\"\n",
      "    }\n",
      "}\n",
      "Epoch: 02\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.189\",\n",
      "        \"metric\": \"0.000\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.170\",\n",
      "        \"metric\": \"0.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 03\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.167\",\n",
      "        \"metric\": \"0.000\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.155\",\n",
      "        \"metric\": \"0.004\"\n",
      "    }\n",
      "}\n",
      "Epoch: 04\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.152\",\n",
      "        \"metric\": \"0.002\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.143\",\n",
      "        \"metric\": \"0.007\"\n",
      "    }\n",
      "}\n",
      "Epoch: 05\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.140\",\n",
      "        \"metric\": \"0.004\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.133\",\n",
      "        \"metric\": \"0.010\"\n",
      "    }\n",
      "}\n",
      "Epoch: 06\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.130\",\n",
      "        \"metric\": \"0.008\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.121\",\n",
      "        \"metric\": \"0.026\"\n",
      "    }\n",
      "}\n",
      "Epoch: 07\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.120\",\n",
      "        \"metric\": \"0.023\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.112\",\n",
      "        \"metric\": \"0.048\"\n",
      "    }\n",
      "}\n",
      "Epoch: 08\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.111\",\n",
      "        \"metric\": \"0.043\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.103\",\n",
      "        \"metric\": \"0.063\"\n",
      "    }\n",
      "}\n",
      "Epoch: 09\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.100\",\n",
      "        \"metric\": \"0.063\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.093\",\n",
      "        \"metric\": \"0.102\"\n",
      "    }\n",
      "}\n",
      "Epoch: 10\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.093\",\n",
      "        \"metric\": \"0.091\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.084\",\n",
      "        \"metric\": \"0.132\"\n",
      "    }\n",
      "}\n",
      "Epoch: 11\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.086\",\n",
      "        \"metric\": \"0.102\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.079\",\n",
      "        \"metric\": \"0.167\"\n",
      "    }\n",
      "}\n",
      "Epoch: 12\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.078\",\n",
      "        \"metric\": \"0.162\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.073\",\n",
      "        \"metric\": \"0.169\"\n",
      "    }\n",
      "}\n",
      "Epoch: 13\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.072\",\n",
      "        \"metric\": \"0.200\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.066\",\n",
      "        \"metric\": \"0.256\"\n",
      "    }\n",
      "}\n",
      "Epoch: 14\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.067\",\n",
      "        \"metric\": \"0.237\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.061\",\n",
      "        \"metric\": \"0.281\"\n",
      "    }\n",
      "}\n",
      "Epoch: 15\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.060\",\n",
      "        \"metric\": \"0.292\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.058\",\n",
      "        \"metric\": \"0.294\"\n",
      "    }\n",
      "}\n",
      "Epoch: 16\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.054\",\n",
      "        \"metric\": \"0.344\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.050\",\n",
      "        \"metric\": \"0.385\"\n",
      "    }\n",
      "}\n",
      "Epoch: 17\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.050\",\n",
      "        \"metric\": \"0.427\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.043\",\n",
      "        \"metric\": \"0.477\"\n",
      "    }\n",
      "}\n",
      "Epoch: 18\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.044\",\n",
      "        \"metric\": \"0.455\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.040\",\n",
      "        \"metric\": \"0.487\"\n",
      "    }\n",
      "}\n",
      "Epoch: 19\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.042\",\n",
      "        \"metric\": \"0.495\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.036\",\n",
      "        \"metric\": \"0.570\"\n",
      "    }\n",
      "}\n",
      "Epoch: 20\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.038\",\n",
      "        \"metric\": \"0.546\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.032\",\n",
      "        \"metric\": \"0.658\"\n",
      "    }\n",
      "}\n",
      "Epoch: 21\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.034\",\n",
      "        \"metric\": \"0.610\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.030\",\n",
      "        \"metric\": \"0.687\"\n",
      "    }\n",
      "}\n",
      "Epoch: 22\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.031\",\n",
      "        \"metric\": \"0.683\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.029\",\n",
      "        \"metric\": \"0.625\"\n",
      "    }\n",
      "}\n",
      "Epoch: 23\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.031\",\n",
      "        \"metric\": \"0.655\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.027\",\n",
      "        \"metric\": \"0.713\"\n",
      "    }\n",
      "}\n",
      "Epoch: 24\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.031\",\n",
      "        \"metric\": \"0.629\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.025\",\n",
      "        \"metric\": \"0.767\"\n",
      "    }\n",
      "}\n",
      "Epoch: 25\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.026\",\n",
      "        \"metric\": \"0.725\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.023\",\n",
      "        \"metric\": \"0.751\"\n",
      "    }\n",
      "}\n",
      "Epoch: 26\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.023\",\n",
      "        \"metric\": \"0.754\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.020\",\n",
      "        \"metric\": \"0.781\"\n",
      "    }\n",
      "}\n",
      "Epoch: 27\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.023\",\n",
      "        \"metric\": \"0.767\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.018\",\n",
      "        \"metric\": \"0.855\"\n",
      "    }\n",
      "}\n",
      "Epoch: 28\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.020\",\n",
      "        \"metric\": \"0.836\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.018\",\n",
      "        \"metric\": \"0.794\"\n",
      "    }\n",
      "}\n",
      "Epoch: 29\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.019\",\n",
      "        \"metric\": \"0.826\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.017\",\n",
      "        \"metric\": \"0.839\"\n",
      "    }\n",
      "}\n",
      "Epoch: 30\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.018\",\n",
      "        \"metric\": \"0.828\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.015\",\n",
      "        \"metric\": \"0.853\"\n",
      "    }\n",
      "}\n",
      "Epoch: 31\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.017\",\n",
      "        \"metric\": \"0.849\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.016\",\n",
      "        \"metric\": \"0.826\"\n",
      "    }\n",
      "}\n",
      "Epoch: 32\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.014\",\n",
      "        \"metric\": \"0.884\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.012\",\n",
      "        \"metric\": \"0.908\"\n",
      "    }\n",
      "}\n",
      "Epoch: 33\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.014\",\n",
      "        \"metric\": \"0.881\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.013\",\n",
      "        \"metric\": \"0.887\"\n",
      "    }\n",
      "}\n",
      "Epoch: 34\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.013\",\n",
      "        \"metric\": \"0.894\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.010\",\n",
      "        \"metric\": \"0.939\"\n",
      "    }\n",
      "}\n",
      "Epoch: 35\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.012\",\n",
      "        \"metric\": \"0.905\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.010\",\n",
      "        \"metric\": \"0.932\"\n",
      "    }\n",
      "}\n",
      "Epoch: 36\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.012\",\n",
      "        \"metric\": \"0.902\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.010\",\n",
      "        \"metric\": \"0.913\"\n",
      "    }\n",
      "}\n",
      "Epoch: 37\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.010\",\n",
      "        \"metric\": \"0.919\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.008\",\n",
      "        \"metric\": \"0.949\"\n",
      "    }\n",
      "}\n",
      "Epoch: 38\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.011\",\n",
      "        \"metric\": \"0.906\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.009\",\n",
      "        \"metric\": \"0.949\"\n",
      "    }\n",
      "}\n",
      "Epoch: 39\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.011\",\n",
      "        \"metric\": \"0.905\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.009\",\n",
      "        \"metric\": \"0.938\"\n",
      "    }\n",
      "}\n",
      "Epoch: 40\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.011\",\n",
      "        \"metric\": \"0.892\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.009\",\n",
      "        \"metric\": \"0.926\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_ds = NumberDataset.from_file('train', processor=proc, target='integer')\n",
    "valid_ds = NumberDataset.from_file('valid', processor=proc, target='integer')\n",
    "train_dl = DataLoader(train_ds, batch_size=10, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=10, collate_fn=collate_fn)\n",
    "r2i_mdl = Seq2SeqWithAttention(vocab_size, hidden_dim=30, dropout=.3)\n",
    "opt = optim.Adam(r2i_mdl.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "metric = Acc(ignore_index=pad_idx)\n",
    "best_r2i_mdl = fit(100, r2i_mdl, train_dl, valid_dl, opt, criterion, metric, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "350b7484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "test_ds = NumberDataset.from_file('test', processor=proc, target='integer')\n",
    "test_dl = DataLoader(test_ds, batch_size=5, collate_fn=collate_fn)\n",
    "_, test_metric = evaluate(best_r2i_mdl, test_dl, criterion, metric)\n",
    "print(test_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "691b94b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IV -> 4 (4)\n",
      "MCXCIII -> 1193 (1193)\n",
      "DXLVIII -> 548 (548)\n",
      "MMMDCXVI -> 3616 (3616)\n",
      "XXI -> 21 (21)\n"
     ]
    }
   ],
   "source": [
    "to_predict = [r for i,r in test_set]\n",
    "preds = predict(best_r2i_mdl, to_predict, proc, collate_fn)\n",
    "for (i,r),pred in zip(test_set,preds):\n",
    "    print(f'{r} -> {pred} ({i})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22e0fe",
   "metadata": {},
   "source": [
    "Pretty cool, even though it's not perfect. Let's collect 4000 more examples and see how it improves both programs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39daa845",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits  = {\n",
    "    'train': 5e3,\n",
    "    'valid': 1e3,\n",
    "    'test': 1e2\n",
    "}\n",
    "\n",
    "make_data(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "060094d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.146\",\n",
      "        \"metric\": \"0.000\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.107\",\n",
      "        \"metric\": \"0.003\"\n",
      "    }\n",
      "}\n",
      "Epoch: 02\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.101\",\n",
      "        \"metric\": \"0.006\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.101\",\n",
      "        \"metric\": \"0.006\"\n",
      "    }\n",
      "}\n",
      "Epoch: 03\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.079\",\n",
      "        \"metric\": \"0.026\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.069\",\n",
      "        \"metric\": \"0.084\"\n",
      "    }\n",
      "}\n",
      "Epoch: 04\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.063\",\n",
      "        \"metric\": \"0.086\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.046\",\n",
      "        \"metric\": \"0.311\"\n",
      "    }\n",
      "}\n",
      "Epoch: 05\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.047\",\n",
      "        \"metric\": \"0.205\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.031\",\n",
      "        \"metric\": \"0.520\"\n",
      "    }\n",
      "}\n",
      "Epoch: 06\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.035\",\n",
      "        \"metric\": \"0.366\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.023\",\n",
      "        \"metric\": \"0.671\"\n",
      "    }\n",
      "}\n",
      "Epoch: 07\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.027\",\n",
      "        \"metric\": \"0.510\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.015\",\n",
      "        \"metric\": \"0.803\"\n",
      "    }\n",
      "}\n",
      "Epoch: 08\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.022\",\n",
      "        \"metric\": \"0.615\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.011\",\n",
      "        \"metric\": \"0.905\"\n",
      "    }\n",
      "}\n",
      "Epoch: 09\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.016\",\n",
      "        \"metric\": \"0.739\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.005\",\n",
      "        \"metric\": \"0.967\"\n",
      "    }\n",
      "}\n",
      "Epoch: 10\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.013\",\n",
      "        \"metric\": \"0.800\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.006\",\n",
      "        \"metric\": \"0.969\"\n",
      "    }\n",
      "}\n",
      "Epoch: 11\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.010\",\n",
      "        \"metric\": \"0.836\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.004\",\n",
      "        \"metric\": \"0.974\"\n",
      "    }\n",
      "}\n",
      "Epoch: 12\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.008\",\n",
      "        \"metric\": \"0.882\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.992\"\n",
      "    }\n",
      "}\n",
      "Epoch: 13\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.007\",\n",
      "        \"metric\": \"0.884\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.996\"\n",
      "    }\n",
      "}\n",
      "Epoch: 14\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.007\",\n",
      "        \"metric\": \"0.906\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"1.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 15\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.005\",\n",
      "        \"metric\": \"0.920\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.993\"\n",
      "    }\n",
      "}\n",
      "Epoch: 16\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.005\",\n",
      "        \"metric\": \"0.931\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"0.999\"\n",
      "    }\n",
      "}\n",
      "Epoch: 17\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.006\",\n",
      "        \"metric\": \"0.919\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.986\"\n",
      "    }\n",
      "}\n",
      "Epoch: 18\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.005\",\n",
      "        \"metric\": \"0.938\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.004\",\n",
      "        \"metric\": \"0.983\"\n",
      "    }\n",
      "}\n",
      "Epoch: 19\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.955\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.013\",\n",
      "        \"metric\": \"0.934\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_ds = NumberDataset.from_file('train', processor=proc)\n",
    "valid_ds = NumberDataset.from_file('valid', processor=proc)\n",
    "train_dl = DataLoader(train_ds, batch_size=10, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=10, collate_fn=collate_fn)\n",
    "i2r_mdl = Seq2SeqWithAttention(vocab_size, hidden_dim=30, dropout=.3)\n",
    "opt = optim.Adam(i2r_mdl.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "metric = Acc(ignore_index=pad_idx)\n",
    "best_i2r_mdl = fit(100, i2r_mdl, train_dl, valid_dl, opt, criterion, metric, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af3a3e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "test_ds = NumberDataset.from_file('test', processor=proc)\n",
    "test_dl = DataLoader(test_ds, batch_size=5, collate_fn=collate_fn)\n",
    "_, test_metric = evaluate(best_i2r_mdl, test_dl, criterion, metric)\n",
    "print(test_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db6a0ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 -> IV (IV)\n",
      "1193 -> MCXCIII (MCXCIII)\n",
      "548 -> DXLVIII (DXLVIII)\n",
      "3616 -> MMMDCXVI (MMMDCXVI)\n",
      "21 -> XXI (XXI)\n"
     ]
    }
   ],
   "source": [
    "to_predict = [i for i,r in test_set]\n",
    "preds = predict(best_i2r_mdl, to_predict, proc, collate_fn)\n",
    "for (i,r),pred in zip(test_set,preds):\n",
    "    print(f'{i} -> {pred} ({r})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d506b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.180\",\n",
      "        \"metric\": \"0.002\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.138\",\n",
      "        \"metric\": \"0.003\"\n",
      "    }\n",
      "}\n",
      "Epoch: 02\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.122\",\n",
      "        \"metric\": \"0.036\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.098\",\n",
      "        \"metric\": \"0.107\"\n",
      "    }\n",
      "}\n",
      "Epoch: 03\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.084\",\n",
      "        \"metric\": \"0.154\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.062\",\n",
      "        \"metric\": \"0.310\"\n",
      "    }\n",
      "}\n",
      "Epoch: 04\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.057\",\n",
      "        \"metric\": \"0.340\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.042\",\n",
      "        \"metric\": \"0.554\"\n",
      "    }\n",
      "}\n",
      "Epoch: 05\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.039\",\n",
      "        \"metric\": \"0.553\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.026\",\n",
      "        \"metric\": \"0.795\"\n",
      "    }\n",
      "}\n",
      "Epoch: 06\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.027\",\n",
      "        \"metric\": \"0.724\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.019\",\n",
      "        \"metric\": \"0.805\"\n",
      "    }\n",
      "}\n",
      "Epoch: 07\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.020\",\n",
      "        \"metric\": \"0.789\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.012\",\n",
      "        \"metric\": \"0.928\"\n",
      "    }\n",
      "}\n",
      "Epoch: 08\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.014\",\n",
      "        \"metric\": \"0.878\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.009\",\n",
      "        \"metric\": \"0.958\"\n",
      "    }\n",
      "}\n",
      "Epoch: 09\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.011\",\n",
      "        \"metric\": \"0.903\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.008\",\n",
      "        \"metric\": \"0.956\"\n",
      "    }\n",
      "}\n",
      "Epoch: 10\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.009\",\n",
      "        \"metric\": \"0.930\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.008\",\n",
      "        \"metric\": \"0.943\"\n",
      "    }\n",
      "}\n",
      "Epoch: 11\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.008\",\n",
      "        \"metric\": \"0.927\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.005\",\n",
      "        \"metric\": \"0.954\"\n",
      "    }\n",
      "}\n",
      "Epoch: 12\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.006\",\n",
      "        \"metric\": \"0.949\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.034\",\n",
      "        \"metric\": \"0.664\"\n",
      "    }\n",
      "}\n",
      "Epoch: 13\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.009\",\n",
      "        \"metric\": \"0.915\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.993\"\n",
      "    }\n",
      "}\n",
      "Epoch: 14\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.979\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.997\"\n",
      "    }\n",
      "}\n",
      "Epoch: 15\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.977\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.984\"\n",
      "    }\n",
      "}\n",
      "Epoch: 16\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.970\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.977\"\n",
      "    }\n",
      "}\n",
      "Epoch: 17\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.005\",\n",
      "        \"metric\": \"0.951\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"0.997\"\n",
      "    }\n",
      "}\n",
      "Epoch: 18\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.004\",\n",
      "        \"metric\": \"0.959\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.013\",\n",
      "        \"metric\": \"0.853\"\n",
      "    }\n",
      "}\n",
      "Epoch: 19\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.968\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"0.996\"\n",
      "    }\n",
      "}\n",
      "Epoch: 20\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.975\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.987\"\n",
      "    }\n",
      "}\n",
      "Epoch: 21\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.968\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"1.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 22\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.005\",\n",
      "        \"metric\": \"0.942\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.994\"\n",
      "    }\n",
      "}\n",
      "Epoch: 23\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.987\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"1.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 24\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.970\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.985\"\n",
      "    }\n",
      "}\n",
      "Epoch: 25\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.982\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"1.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 26\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.982\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"0.997\"\n",
      "    }\n",
      "}\n",
      "Epoch: 27\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.973\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.000\",\n",
      "        \"metric\": \"1.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 28\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.984\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.000\",\n",
      "        \"metric\": \"1.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 29\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"0.994\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.000\",\n",
      "        \"metric\": \"1.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 30\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.967\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.005\",\n",
      "        \"metric\": \"0.919\"\n",
      "    }\n",
      "}\n",
      "Epoch: 31\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.972\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.000\",\n",
      "        \"metric\": \"1.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 32\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"0.997\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.000\",\n",
      "        \"metric\": \"1.000\"\n",
      "    }\n",
      "}\n",
      "Epoch: 33\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"0.990\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"0.997\"\n",
      "    }\n",
      "}\n",
      "Epoch: 34\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.004\",\n",
      "        \"metric\": \"0.951\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.002\",\n",
      "        \"metric\": \"0.981\"\n",
      "    }\n",
      "}\n",
      "Epoch: 35\n",
      "\t{\n",
      "    \"train\": {\n",
      "        \"loss\": \"0.003\",\n",
      "        \"metric\": \"0.964\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "        \"loss\": \"0.001\",\n",
      "        \"metric\": \"0.995\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_ds = NumberDataset.from_file('train', processor=proc, target='integer')\n",
    "valid_ds = NumberDataset.from_file('valid', processor=proc, target='integer')\n",
    "train_dl = DataLoader(train_ds, batch_size=10, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=10, collate_fn=collate_fn)\n",
    "r2i_mdl = Seq2SeqWithAttention(vocab_size, hidden_dim=30, dropout=.3)\n",
    "opt = optim.Adam(r2i_mdl.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "metric = Acc(ignore_index=pad_idx)\n",
    "best_r2i_mdl = fit(100, r2i_mdl, train_dl, valid_dl, opt, criterion, metric, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58143d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_ds = NumberDataset.from_file('test', processor=proc, target='integer')\n",
    "test_dl = DataLoader(test_ds, batch_size=5, collate_fn=collate_fn)\n",
    "_, test_metric = evaluate(best_r2i_mdl, test_dl, criterion, metric)\n",
    "print(test_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40a8c5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IV -> 4 (4)\n",
      "MCXCIII -> 1193 (1193)\n",
      "DXLVIII -> 548 (548)\n",
      "MMMDCXVI -> 3616 (3616)\n",
      "XXI -> 21 (21)\n"
     ]
    }
   ],
   "source": [
    "to_predict = [r for i,r in test_set]\n",
    "preds = predict(best_r2i_mdl, to_predict, proc, collate_fn)\n",
    "for (i,r),pred in zip(test_set,preds):\n",
    "    print(f'{r} -> {pred} ({i})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e9770",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this post, I've described two software paradigms: imperative programming (software 1.0) and programming by example (software 2.0). I've argued that, because they currently tend to focus on non-overlapping problems, they are not easily seen as substitutes of each other. I used the (silly) example of converting Roman numerals into integers and vice-versa to illustrate that substitutability. In the process, I've highlighted the pros and cons of each paradigm. Because software 2.0 is \"self-writing\", the skills to \"develop\" that software (curate datasets of examples) are cheaper to source. In that sense, neural nets constitute \"meta-software\", or software that produces software. Of course, the neural nets themselves have to be coded imperatively, but the argument is that they are destined to become commodities. And because they are more easily multi-purpose, the fixed cost of coding them is amortized over multiple problems. In the above example, this was illustrated by repurposing the same neural net to solve a task that, in the software 1.0 paradigm, requires a complete rewrite of the software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3da4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
